{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "user_preferences = {\n",
    "    \"interests\": [\"Technology\", \"Science\", \"Art\"],\n",
    "    \"content_types\": [\"Article\", \"Tutorial\"],\n",
    "    \"text_format\": \"summary\"\n",
    "}\n",
    "\n",
    "mock_content = [\n",
    "    {\"title\": \"The Future of Tech: Innovations to Watch\", \"category\": \"Technology\", \"type\": \"Article\", \"summary\": \"Explore the cutting-edge technologies shaping our future, from AI advancements to sustainable energy solutions.\"},\n",
    "    {\"title\": \"Beginner's Guide to Digital Art\", \"category\": \"Art\", \"type\": \"Tutorial\", \"summary\": \"Step into the world of digital art with this comprehensive tutorial for beginners. Learn about tools, techniques, and tips to start your digital art journey.\"},\n",
    "    {\"title\": \"Understanding Quantum Computing\", \"category\": \"Science\", \"type\": \"Article\", \"summary\": \"Dive into the complex world of quantum computing and discover how it's set to revolutionize technology with unparalleled computational power.\"},\n",
    "    {\"title\": \"The Renaissance Art Movement\", \"category\": \"Art\", \"type\": \"Summary\", \"summary\": \"A brief overview of the Renaissance period, highlighting its most influential artists and their timeless masterpieces.\"},\n",
    "    {\"title\": \"DIY Home Robotics Projects\", \"category\": \"Technology\", \"type\": \"Tutorial\", \"summary\": \"Get hands-on with technology by exploring these simple yet fascinating DIY robotics projects you can build at home.\"},\n",
    "    {\"title\": \"The Mysteries of Dark Matter\", \"category\": \"Science\", \"type\": \"Article\", \"summary\": \"Join us on a journey through the cosmos as we explore the enigmatic nature of dark matter and its role in the universe.\"},\n",
    "    {\"title\": \"Creative Writing Tips for Beginners\", \"category\": \"Art\", \"type\": \"Tutorial\", \"summary\": \"Unlock your creative potential with these essential writing tips, perfect for aspiring authors starting their journey.\"},\n",
    "    {\"title\": \"Innovative Eco-Friendly Technologies\", \"category\": \"Technology\", \"type\": \"Summary\", \"summary\": \"Discover how innovative technologies are paving the way for a more sustainable future, focusing on renewable energy sources and eco-friendly solutions.\"},\n",
    "    {\"title\": \"Exploring the Human Genome\", \"category\": \"Science\", \"type\": \"Summary\", \"summary\": \"An insightful summary of what the human genome project has taught us and how it continues to influence scientific research and medical advancements.\"},\n",
    "    {\"title\": \"Photography Basics: Capturing the Moment\", \"category\": \"Art\", \"type\": \"Article\", \"summary\": \"Learn the fundamentals of photography with this guide, from understanding your camera to composing the perfect shot.\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_user_preferences(interests, content_types):\n",
    "    return {\n",
    "        \"interests\": interests,\n",
    "        \"content_types\": content_types\n",
    "    }\n",
    "\n",
    "# Example of setting preferences\n",
    "user_preferences = set_user_preferences([\"Technology\", \"Art\"], [\"Article\", \"Tutorial\"])\n",
    "\n",
    "print(user_preferences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_content(preferences):\n",
    "    # Filter content based on user preferences\n",
    "    filtered_content = [content for content in mock_content if content[\"category\"] in preferences[\"interests\"] and content[\"type\"] in preferences[\"content_types\"]]\n",
    "    return filtered_content\n",
    "\n",
    "# Example usage\n",
    "personalized_content = generate_content(user_preferences)\n",
    "print(personalized_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI who writes a summary of the article.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Say this is a test\"},\n",
    "        ]\n",
    "        ,\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANGCHAIN PROTOTYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "def generate_content_with_langchain(interest, content_type):\n",
    "    # Craft a prompt based on user interest and desired content type\n",
    "    template = \"\"\"Question: {question}\n",
    "\n",
    "    Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "    question_template = \"\"\"Create a {content_type} about {interest}. \n",
    "    It should be engaging and informative.\"\"\"\n",
    "    question = question_template.format(content_type=content_type, interest=interest)\n",
    "    # Use Langchain's OpenAI LLM to generate content\n",
    "    # Note: Adjust the parameters according to your requirements\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    # Extracting the generated content\n",
    "    generated_content = llm_chain.run(question)\n",
    "\n",
    "    return {\n",
    "        \"title\": f\"{interest} {content_type}\",\n",
    "        \"content\": generated_content\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "interest = \"Sustainable Living\"\n",
    "content_type = \"Summary\"\n",
    "generated_content = generate_content_with_langchain(interest, content_type)\n",
    "print(generated_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_content[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Imports main tools:\n",
    "from trulens_eval import TruChain, Feedback, Tru\n",
    "tru = Tru()\n",
    "tru.reset_database()\n",
    "\n",
    "# Imports from langchain to build app\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback.provider import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "# Initialize provider class\n",
    "openai = OpenAI()\n",
    "\n",
    "# select context to be used in feedback. the location of context is app specific.\n",
    "from trulens_eval.app import App\n",
    "context = App.select_context(rag_chain)\n",
    "\n",
    "from trulens_eval.feedback import Groundedness\n",
    "grounded = Groundedness(groundedness_provider=OpenAI())\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons)\n",
    "    .on(context.collect()) # collect context chunks into a list\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(openai.relevance).on_input_output()\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(openai.qs_relevance)\n",
    "    .on_input()\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_recorder = TruChain(rag_chain,\n",
    "    app_id='Chain1_ChatApplication',\n",
    "    feedbacks=[f_qa_relevance, f_context_relevance, f_groundedness])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder as recording:\n",
    "    llm_response = rag_chain.invoke(\"What is Task Decomposition?\")\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The record of the app invocation can be retrieved from the `recording`:\n",
    "\n",
    "rec = recording.get() # use .get if only one record\n",
    "# recs = recording.records # use .records if multiple\n",
    "\n",
    "display(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results of the feedback functions can be rertireved from\n",
    "# `Record.feedback_results` or using the `wait_for_feedback_result` method. The\n",
    "# results if retrieved directly are `Future` instances (see\n",
    "# `concurrent.futures`). You can use `as_completed` to wait until they have\n",
    "# finished evaluating or use the utility method:\n",
    "\n",
    "for feedback, feedback_result in rec.wait_for_feedback_results().items():\n",
    "    print(feedback.name, feedback_result.result)\n",
    "\n",
    "# See more about wait_for_feedback_results:\n",
    "# help(rec.wait_for_feedback_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[\"Chain1_ChatApplication\"])\n",
    "\n",
    "records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[\"Chain1_ChatApplication\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.stop_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_exa import ExaSearchRetriever, TextContentsOptions\n",
    "# This is for the exa search retriever"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
